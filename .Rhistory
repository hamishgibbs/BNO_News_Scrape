wp_pop = wp_village_populations)
write.csv(village_populations_df, 'C:/HG/Projects/Population Estimation/Lake Victoria/Village_Population_Estimates_22.csv')
write.csv(village_populations_df, '/Volumes/HOMEQ/eph/lshhg2/Projects/Population_Estimation/Lake Victoria/Results/village_populations_df.csv')
tanzania_villages@data$Area
tz = c(min(tanzania_villages@data$Area),
mean(tanzania_villages@data$Area),
max(tanzania_villages@data$Area))
tanzania_villages@data$Area
tz = c(min(as.numeric(as.character(tanzania_villages@data$Area))),
mean(as.numeric(as.character(tanzania_villages@data$Area))),
max(as.numeric(as.character(tanzania_villages@data$Area))))
tanzania_villages@data$Area
tz = c(min(as.numeric(as.character(tanzania_villages@data$Area))),
mean(as.numeric(as.character(tanzania_villages@data$Area))),
max(as.numeric(as.character(tanzania_villages@data$Area))))
ug = c(min(as.numeric(as.character(uganda_villages@data$Area))),
mean(as.numeric(as.character(uganda_villages@data$Area))),
max(as.numeric(as.character(uganda_villages@data$Area))))
kn = c(min(as.numeric(as.character(kenya_villages@data$Area))),
mean(as.numeric(as.character(kenya_villages@data$Area))),
max(as.numeric(as.character(kenya_villages@data$Area))))
rbind(tz, ug, kn)
area_df = rbind(tz, ug, kn)
colnames(area_df) = c('min', 'mean', 'max')
write.csv(area_df, '/Volumes/HOMEQ/eph/lshhg2/Projects/Population_Estimation/Lake Victoria/Results/area_df')
write.csv(area_df, '/Volumes/HOMEQ/eph/lshhg2/Projects/Population_Estimation/Lake Victoria/Results/area_df.csv')
kn
ug
tz = c(min(as.numeric(as.character(tanzania_villages@data$Area))),
mean(as.numeric(as.character(tanzania_villages@data$Area))),
max(as.numeric(as.character(tanzania_villages@data$Area))))
tz
ug = c(min(as.numeric(as.character(uganda_villages@data$Area))),
mean(as.numeric(as.character(uganda_villages@data$Area))),
max(as.numeric(as.character(uganda_villages@data$Area))))
ug
kn = c(min(as.numeric(as.character(kenya_villages@data$Area))),
mean(as.numeric(as.character(kenya_villages@data$Area))),
max(as.numeric(as.character(kenya_villages@data$Area))))
kn
area_df = rbind(tz, ug, kn)
colnames(area_df) = c('min', 'mean', 'max')
write.csv(area_df, '/Volumes/HOMEQ/eph/lshhg2/Projects/Population_Estimation/Lake Victoria/Results/area_df.csv')
area_df
sum(as.numeric(as.character(kenya_villages@data$Area)))
sum(as.numeric(as.character(tanzania_villages@data$Area)))
sum(as.numeric(as.character(uganda_villages@data$Area)))
sum(as.numeric(as.character(kenya_villages@data$Area)))
sum(as.numeric(as.character(tanzania_villages@data$Area))) + sum(as.numeric(as.character(uganda_villages@data$Area))) + sum(as.numeric(as.character(kenya_villages@data$Area)))
#REPEAT WITH MAJOR CITIES AND VILLAGES EXCLUDED - inverse mask by polygons
major_cities = shapefile('/Volumes/HOMEQ/eph/lshhg2/Projects/Population_Estimation/Lake Victoria/Cities/towns.shp')
tanzania_villages = shapefile('/Volumes/HOMEQ/eph/lshhg2/Projects/Population_Estimation/Lake Victoria/All_villages/All_villages_2/WGS84/tanzania_vil.shp')
uganda_villages = shapefile('/Volumes/HOMEQ/eph/lshhg2/Projects/Population_Estimation/Lake Victoria/All_villages/All_villages_2/WGS84/uganda_vil.shp')
kenya_villages = shapefile('/Volumes/HOMEQ/eph/lshhg2/Projects/Population_Estimation/Lake Victoria/All_villages/All_villages_2/WGS84/kenya_vil.shp')
wp_rasters = c(wp_tanzania, wp_uganda, wp_kenya)
fb_rasters = c(fb_tanzania, fb_uganda, fb_kenya)
villages = c(tanzania_villages, uganda_villages, kenya_villages)
plot(major_cities)
wp_cities_masked = c()
fb_cities_masked = c()
i = 1
for (raster in wp_rasters){
wp_raster = wp_rasters[[i]]
fb_raster = fb_rasters[[i]]
#crop to largest buffer
cropped_wp = crop(wp_raster, extent(buffer_10k))
cropped_fb = crop(fb_raster, extent(buffer_10k))
wp_mask = mask(cropped_wp, major_cities, inverse=T)
wp_mask = mask(wp_mask, villages[[i]], inverse=T)
fb_mask = mask(cropped_fb, major_cities, inverse=T)
fb_mask = mask(fb_mask, villages[[i]], inverse=T)
wp_cities_masked = append(wp_cities_masked, wp_mask)
fb_cities_masked = append(fb_cities_masked, fb_mask)
i = i + 1
}
buffers = c(buffer_1k, buffer_5k, buffer_10k)
buffer_names = c("buffer_1k", "buffer_5k", "buffer_10k")
countries = c('Tanzania', 'Uganda', 'Kenya')
wp_pop_est_no_cities = c()
fb_pop_est_no_cities = c()
country_index = c()
buffer_index = c()
i = 1
for (country in countries){
country_wp = wp_cities_masked[[i]]
country_fb = fb_cities_masked[[i]]
print(country)
a = 1
for (buffer in buffers){
print(paste(buffer_names[a], ': World Pop', sep=''))
cropped_wp = crop(country_wp, extent(buffer))
cropped_wp = mask(cropped_wp, buffer)
cropped_wp = trim(cropped_wp)
wp_pop_est = cellStats(cropped_wp, sum)
print(paste(buffer_names[a], ' Facebook', sep=''))
cropped_fb = crop(country_fb, extent(buffer))
cropped_fb = mask(cropped_fb, buffer)
cropped_fb = trim(cropped_fb)
fb_pop_est = cellStats(cropped_fb, sum)
wp_pop_est_no_cities = append(wp_pop_est_no_cities, wp_pop_est)
fb_pop_est_no_cities = append(fb_pop_est_no_cities, fb_pop_est)
country_index = append(country_index, countries[i])
buffer_index = append(buffer_index, buffer_names[a])
a = a + 1
}
i = i + 1
}
with_no_cities_df = data.frame(wp_pop=wp_pop_est_no_cities, fb_pop=fb_pop_est_no_cities, country=country_index, buffer=buffer_index)
write.csv(with_no_cities_df, '/Volumes/HOMEQ/eph/lshhg2/Projects/Population_Estimation/Lake Victoria/Results/with_no_cities_or_towns_df.csv')
pacman::p_load(tidyverse)
connect = read_rds('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Connectivity/connectivity_draft.rds')
connect
pop = read_rds('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Connectivity/shp_pop.rds')
pop
#make centroids df to make things easier
class(pop)
pacman::p_load(tidyverse)
connect = read_rds('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Connectivity/connectivity_draft.rds')
pacman::p_load(tidyverse)
connect = read_rds('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Connectivity/connectivity_draft.rds')
ggplot(pop)
pacman::p_load(tidyverse, ggplot)
pacman::p_load(tidyverse, ggplot2)
ggplot(pop)
connect = read_rds('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Connectivity/connectivity_draft.rds')
connect
pop = read_rds('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Connectivity/shp_pop.rds')
pop
pop
pacman::p_load(tidyverse, ggplot2, sf)
ggplot(pop)
pacman::p_load(threejs,
grDevices,
geosphere,
ggspatial,
rnaturalearth,
sf,
raster,
tidyverse)
pop
pop = read_rds('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Connectivity/shp_pop.rds')
pop
ggplot(pop)
class(pop)
ggplot() +
geom_sf(data = counties, fill = NA, color = gray(.5))
ggplot() +
geom_sf(data = pop, fill = NA, color = gray(.5))
ggplot() +
geom_sf(data = pop, fill = NA, color = gray(.5))
city_df = filter(connect, test_codes[1])
city_df = as_tibble(cbind(nms = names(city_df), t(city_df)))
city_df = filter(connect, test_codes[1])
city_df = as_tibble(cbind(nms = names(city_df), t(city_df)))
#get codes from population dataframe
code = pop %>%
slice(test_index) %>%
select(CNTY_CODE)
test_codes = code$CNTY_CODE
test_cities = c('beijing', 'shanghai', 'tianxin', 'changsha', 'weizhou', 'chengdu', 'shenzhen', 'guangzhou')
test_cities = c('beijing', 'shanghai', 'tianxin', 'changsha', 'weizhou', 'chengdu', 'shenzhen', 'guangzhou')
readRDS('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Connectivity/connectivity_draft.rds')
connect = read_rds('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Connectivity/connectivity_draft.rds')
pacman::p_load(tidyverse, ggplot2, sf)
connect = read_rds('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Connectivity/connectivity_draft.rds')
connect
pop = read_rds('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Connectivity/shp_pop.rds')
pop
test_cities = c('beijing', 'shanghai', 'tianxin', 'changsha', 'weizhou', 'chengdu', 'shenzhen', 'guangzhou')
test_index = c()
for (city in test_cities){
test_index = append(test_index, grep(city, pop$PYNAME, ignore.case = TRUE))
}
#get codes from population dataframe
code = pop %>%
slice(test_index) %>%
select(CNTY_CODE)
test_codes = code$CNTY_CODE
test_codes
city_df = filter(connect, test_codes[1])
city_df = filter(connect, from == test_codes[1])
city_df = as_tibble(cbind(nms = names(city_df), t(city_df)))
city_df = filter(connect, from == test_codes[1])
city_df2 = as_tibble(cbind(nms = names(city_df), t(city_df)))
city_df2
city_df2[3:length(city_df2)]
city_df2[3:length(city_df2),]
city_df2
city_df2[4:length(city_df2$V2),]
pop[pop$CNTY_CODE == test_codes[1],]
pop[pop$CNTY_CODE == test_codes[1], 'flow']
pop$flow = NA
pop[pop$CNTY_CODE == test_codes[1], 'flow']
i = 0
pop$flow = NA
i = 0
for (code in city_df2){
pop[pop$CNTY_CODE == code, 'flow'] = city_df2[i, 'V2']
i = i + 1
}
pop[pop$CNTY_CODE == code, 'flow']
code
pop$flow = NA
i = 0
for (code in city_df2$nms){
pop[pop$CNTY_CODE == code, 'flow'] = city_df2[i, 'V2']
i = i + 1
}
pop
ggplot() +
geom_sf(data = pop, aes(fill = flow, color = gray(.5)))
ggplot() +
geom_sf(data = pop, aes(fill = flow)) +
scale_fill_viridis_c(trans = "sqrt", alpha = .4)
ggplot() +
geom_sf(data = pop, aes(fill = flow)) +
scale_fill_viridis_c(alpha = .4)
ggplot() +
geom_sf(data = pop, aes(fill = flow))
pop$flow
drop_na(pop)
pop$flow
pop = drop_na(pop)
pop$flow
ggplot() +
geom_sf(data = pop, aes(fill = flow))
ggplot() +
geom_sf(data = pop, fill = flow)
ggplot() +
geom_sf(data = pop, fill = pop$flow)
ggplot() +
geom_sf(data = pop, fill = pop$flow) +
scale_colour_gradient(low = "white", high = "black")
ggplot() +
geom_sf(data = pop, aes(fill = pop$flow)) +
scale_colour_gradient(low = "white", high = "black")
ggplot() +
geom_sf(data = pop, aes(fill = pop$flow)) +
coord_map()
install.packages('mapproj')
ggplot() +
geom_sf(data = pop, aes(fill = pop$flow)) +
coord_map()
ggplot() +
geom_sf(data = pop, aes(fill = pop$flow)) +
coord_sf()
ggplot() +
geom_sf(data = pop, aes(fill = avg_age_15,
x = long,
y = lat,
group = group)) +
coord_sf()
ggplot() +
geom_sf(data = pop, aes(fill = avg_age_15,
x = lng,
y = lat,
group = group)) +
coord_sf()
ggplot() +
geom_sf(data = pop, aes(fill = avg_age_15,
group = group)) +
coord_sf()
ggplot() +
geom_sf(data = pop, aes(fill = flow,
group = group)) +
coord_sf()
ggplot() +
geom_sf(data = pop, aes(fill = flow)) +
coord_sf()
plot(pop , col=pop$flow,  bg = "#A6CAE0")
pacman::p_load(tidyverse, ggplot2, sf, RColorBrewer)
plot(pop$flow, col=pop$flow,  bg = "#A6CAE0")
my_colors <- brewer.pal(9, "Reds")
cbndata = read_rds('/Users/hamishgibbs/Dropbox/nCov-2019/data_sources/case_data/CBNDATA_Scrape/cbndata_features.rds')
pacman::p_load(jsonlite,
tidyverse,
dplyr,
geojsonio,
leaflet)
cbndata = read_rds('/Users/hamishgibbs/Dropbox/nCov-2019/data_sources/case_data/CBNDATA_Scrape/cbndata_features.rds')
cbndata = read_rds('/Users/hamishgibbs/Dropbox/nCov-2019/data_sources/case_data/CBNDATA_Scrape/cbndata_features.rds')
cbndata %>%
leaflet() %>%
addTiles() %>%
addCircleMarkers(radius=1,
color = 'k',
stroke = FALSE)
cbndata %>%
leaflet() %>%
addTiles() %>%
addCircleMarkers(radius=2,
color = 'k',
stroke = FALSE)
cbndata %>%
leaflet() %>%
addTiles() %>%
addCircleMarkers(radius=2,
color = 'k',
stroke = FALSE,
fillOpacity = 1)
pacman::p_load(jsonlite,
tidyverse,
dplyr,
geojsonio,
leaflet,
geojson_sf)
geojson_sf('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Singapore_Data/singapore_data.geojson')
sf <- geojson_sf(system.file("/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Singapore_Data", "singapore_data.geojson", package = "geojsonsf"))
sf <- geojson_sf(system.file("/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Singapore_Data/", "singapore_data.geojson", package = "geojsonsf"))
sf <- geojson_sf(system.file("/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Singapore_Data/singapore_data.geojson", package = "geojsonsf"))
geojson_read('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Singapore_Data/singapore_data.geojson')
geojson = geojson_read('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Singapore_Data/singapore_data.geojson')
geojson_sf(geojson)
system.file("/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Singapore_Data/singapore_data.geojson", package = "geojsonsf")
system.file("/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Singapore_Data", "singapore_data.geojson", package = "geojsonsf")
geojson_sf('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Singapore_Data/singapore_data.geojson')
geojson_sf('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Singapore_Data/singapore_data.geojson')
geojson_sf('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Singapore_Data/singapore_data.geojson')
pacman::p_load(jsonlite,
tidyverse,
dplyr,
geojsonio,
leaflet,
geojson_sf)
geojson_sf('/Users/hamishgibbs/Documents/nCOV-2019/Web_Scraping/Singapore_Data/singapore_data.geojson')
list_elements = bn_list_elements(read_html(url_1))
pacman::p_load(rvest,
tidyverse)
source("./Functions/bn_list_elements.R")
setwd("~/Documents/nCOV-2019/Connectivity_Analysis/BNO_News_Scrape")
pacman::p_load(rvest,
tidyverse)
source("./Functions/bn_list_elements.R")
source("./Functions/bn_return_raw_data_table.R")
source("./Functions/bn_strip_time_source.R")
url_1 = 'https://bnonews.com/index.php/2020/01/timeline-coronavirus-epidemic/'
list_elements = bn_list_elements(read_html(url_1))
raw_data = bn_return_raw_data_table(list_elements)
source("./Functions/bn_list_elements.R")
source("./Functions/bn_return_raw_data_table.R")
source("./Functions/bn_strip_time_source.R")
url_1 = 'https://bnonews.com/index.php/2020/01/timeline-coronavirus-epidemic/'
list_elements = bn_list_elements(read_html(url_1))
raw_data = bn_return_raw_data_table(list_elements)
raw_data
View(raw_data)
list_elements = bn_list_elements(read_html(url_1))
dates = c()
text = c()
for (node in list_elements){
if ((html_name(node) %in% c('h4', 'p')) & (!grepl('The following is a timeline of new cases in China', html_text(node)))){
date = as.Date(html_text(node), '%d %B')
print(date)
}
if (html_name(node) == 'li'){
li = html_text(node)
time = parse_datetime(paste0(date, ' ' , substr(li, 1, 5)), format = '%Y-%m-%d %I:%M')
print(time)
}
if (!is.na(time) & (!html_text(node) %in% c('Source', 'Source 1', 'Source 2'))){
dates = append(dates, date)
text = append(text, html_text(node))
}
}
dates
dates = c()
text = c()
for (node in list_elements){
if ((html_name(node) %in% c('h4', 'p')) & (!grepl('The following is a timeline of new cases in China', html_text(node)))){
date = as.Date(html_text(node), '%d %B')
print(date)
}
if (html_name(node) == 'li'){
li = html_text(node)
time = parse_datetime(paste0(date, ' ' , substr(li, 1, 5)), format = '%Y-%m-%d %I:%M')
print(time)
}
if (!is.na(time) & (!html_text(node) %in% c('Source', 'Source 1', 'Source 2'))){
dates = append(dates, date)
text = append(text, html_text(node))
}
}
dates[1]
dates[2]
dates[33]
dates[334]
dates[3344]
data = tibble(date = dates, text = text)
View(data)
list_elements = bn_list_elements(read_html(url_1))
raw_data = bn_return_raw_data_table(list_elements)
raw_data
bn_strip_time(raw_data)
text
list_elements = bn_list_elements(read_html(url_1))
for (i in 1:100000){print(i)}
raw_data = bn_return_raw_data_table(list_elements)
bn_strip_time(raw_data)
source("./Functions/bn_strip_time_source.R")
source("./Functions/bn_list_elements.R")
source("./Functions/bn_return_raw_data_table.R")
source("./Functions/bn_strip_time_source.R")
raw_data = bn_return_raw_data_table(list_elements)
bn_strip_time(raw_data)
list_elements = bn_list_elements(read_html(url_1))
raw_data = bn_return_raw_data_table(list_elements)
bn_strip_time(raw_data)
source("./Functions/bn_strip_time_source.R")
bn_strip_time(raw_data)
t
t
source("./Functions/bn_strip_time_source.R")
bn_strip_time(raw_data)
full_text
source("./Functions/bn_strip_time_source.R")
bn_strip_time(raw_data)
source("./Functions/bn_strip_time_source.R")
bn_strip_time(raw_data)
source("./Functions/bn_strip_time_source.R")
bn_strip_time(raw_data)
source("./Functions/bn_strip_time_source.R")
bn_strip_time(raw_data)
text = "a (SOURCE)"
class(text)
text
str_remove(text, "a")
str_remove(text, "(SOURCE")
str_remove(text, "(SOURCE)")
str_remove(text, "((SOURCE))")
str_remove(text, "s/([()])//g")
str_remove(text, "s/(SOURCE//g")
str_remove(text, "s/(SOURCE)//g")
source("./Functions/bn_strip_time_source.R")
bn_strip_time(raw_data)
text = "1 new case in Qatar. The patient is a Qatari citizen who was evacuated from Iran. (Source)"
text
gsub("is", "", text)
source("./Functions/bn_strip_time_source.R")
bn_strip_time(raw_data)
gsub(" (Source)","", full_text)
text
gsub(" (Source)","", text)
gsub("(Source)","", text)
gsub("\(Source)","", text)
gsub("/(Source)","", text)
str_split(text, '.')
str_split(text)
text
gsub("\\s*\\([^\\)]+\\)","", text)
gsub("\\s*\\([SOURCE)]+\\)","", text)
gsub("\\s*\\([Source)]+\\)","", text)
source("./Functions/bn_strip_time_source.R")
list_elements = bn_list_elements(read_html(url_1))
raw_data = bn_return_raw_data_table(list_elements)
bn_strip_time(raw_data)
bn_strip_time(raw_data) %>%
separate(text, sep='.')
bn_strip_time(raw_data)
time_strip = bn_strip_time(raw_data)
time_strip
time_strip[!grepl("Total at the end of the day", df$Name),]
time_strip[!grepl("Total at the end of the day", time_strip$text),]
View(time_strip)
source("./Functions/bn_list_elements.R")
source("./Functions/bn_return_raw_data_table.R")
source("./Functions/bn_strip_time_source.R")
url_1 = 'https://bnonews.com/index.php/2020/01/timeline-coronavirus-epidemic/'
list_elements = bn_list_elements(read_html(url_1))
raw_data = bn_return_raw_data_table(list_elements)
time_strip = bn_strip_time(raw_data)
time_strip
View(time_strip)
text = " ahsg (Source 1)"
str_replace(text, " (Source 1)", "")
source("./Functions/bn_strip_time_source.R")
time_strip = bn_strip_time(raw_data)
View(raw_data)
?grep
source("./Functions/bn_extract_new_cases.R")
source("./Functions/bn_extract_new_cases.R")
source("./Functions/bn_extract_new_cases.R")
source("./Functions/bn_extract_new_cases.R")
time_strip = bn_strip_time(raw_data)
time_strip
bn_extract_new_cases(time_strip)
source("./Functions/bn_list_elements.R")
source("./Functions/bn_return_raw_data_table.R")
source("./Functions/bn_strip_time_source.R")
source("./Functions/bn_extract_new_cases.R")
bn_extract_new_cases(time_strip)
source("./Functions/bn_extract_new_cases.R")
bn_extract_new_cases(time_strip)
source("./Functions/bn_extract_new_cases.R")
bn_extract_new_cases(time_strip)
source("./Functions/bn_extract_new_cases.R")
source("./Functions/bn_extract_new_cases.R")
bn_extract_new_cases(time_strip)
source("./Functions/bn_extract_new_cases.R")
bn_extract_new_cases(time_strip)
source("./Functions/bn_extract_new_cases.R")
bn_extract_new_cases(time_strip)
text = "12 new cases today"
grep("\\bnew case\\", text)
grep("\\bnew case\", text)
grep("\\bnew case\", text)
grep("\\bnew case\", text)
grep("\\bnew case\", text)
text = "12 new cases today"
grep("\\bnew case", text)
grep("\\d \bnew case", text)
grep("\\d\bnew case", text)
grep("\\bnew case", text)
